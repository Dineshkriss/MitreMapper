{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f2851506e34590966d597c18f647f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karun\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\karun\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f618b89c5854386a85bf87863f1f25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b85862a8a849748ca66fcbad97b534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08af38bacf8f4319957e67d288cd4216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f8b5ae1d42410d9a9784e5bf64867e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7068876028060913\n",
      "Epoch 2, Loss: 0.6634136885404587\n",
      "Epoch 3, Loss: 0.5801873505115509\n",
      "Training Complete!\n",
      "Text: RT Not out of the woods yet @dennisu24025937 \n",
      " #cybersecurity  #cyberthreats  #ransomware  #cyberinsurance  #socialengineering https://lnkd.in/d_vCpm3(https://t.co/m9ieXDlqPK)\n",
      "Threat Intelligence: False\n",
      "Mapped MITRE ATT&CK TTP: No MITRE ATT&CK mapping found\n",
      "\n",
      "Text: RT Hackers Are Targeting Microsoft Exchange Servers With Ransomware!!\n",
      "It did not take long since last week to do that! \n",
      "#DFIR #TrufflepigForensics #DigitalForensics #CyberSecurity #ITForensics #MemoryForensics #Microsoft #attacks #Exchange #Exploit #Ransomware https://twitter.com/phillip_misner/status/1370197696280027136(https://t.co/wM4FmUpPZ9)\n",
      "Threat Intelligence: True\n",
      "Mapped MITRE ATT&CK TTP: No MITRE ATT&CK mapping found\n",
      "\n",
      "Text: RT New B-Side episode: @israel_barak an expert on cyber-warfare and CISO @cybereason on the recent Microsoft Exchange hack that hit thousands of organizations worldwide: What happened? what vulnerabilities were exploited in the attack? @ranlevi https://malicious.life/episode/episode-108/(https://t.co/SBuLDSwY1u) https://twitter.com/MaliciousLife/status/1373989574804975621/photo/1(https://t.co/AemV1ZhjNF)\n",
      "Threat Intelligence: False\n",
      "Mapped MITRE ATT&CK TTP: No MITRE ATT&CK mapping found\n",
      "\n",
      "Text: RT A new PoC was released this weekend for the ProxyLogon Microsoft Exchange vulnerabilities that requires very little changes to perform RCE on vulnerable servers.\n",
      "Threat Intelligence: True\n",
      "Mapped MITRE ATT&CK TTP: No MITRE ATT&CK mapping found\n",
      "\n",
      "Text: Wrote up a quick HOWTO on using Intrigue Core to find Exchange &amp; CVE-2021-26855 across your organization / partners / subsidiaries. https://twitter.com/Intrigueio/status/1369124850984493057(https://t.co/Hgcykh1aIZ)\n",
      "Threat Intelligence: False\n",
      "Mapped MITRE ATT&CK TTP: T1190 - Exploit Public-Facing Application\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hardcoded MITRE ATT&CK Mapping for CVEs\n",
    "MITRE_MAPPING = {\n",
    "    \"CVE-2021-26855\": \"T1190 - Exploit Public-Facing Application\",\n",
    "    \"CVE-2021-26857\": \"T1210 - Remote Code Execution\",\n",
    "    \"CVE-2021-26858\": \"T1072 - Remote Services\",\n",
    "    \"CVE-2021-27065\": \"T1203 - Exploitation for Client Execution\",\n",
    "}\n",
    "\n",
    "def extract_mitre_ttp(text):\n",
    "    for cve, ttp in MITRE_MAPPING.items():\n",
    "        if cve in text:\n",
    "            return ttp\n",
    "    return \"No MITRE ATT&CK mapping found\"\n",
    "\n",
    "# Load dataset\n",
    "train_path = \"df_train.csv\"\n",
    "test_path = \"df_test.csv\"\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# Define dataset class\n",
    "class ThreatIntelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Tokenizer\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "dataset_train = ThreatIntelDataset(df_train['text'].tolist(), df_train['label'].tolist(), TOKENIZER)\n",
    "dataset_test = ThreatIntelDataset(df_test['text'].tolist(), df_test['label'].tolist(), TOKENIZER)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=8, shuffle=False)\n",
    "\n",
    "# Load Pretrained Model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader_train)}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "test_texts = df_test['text'].tolist()\n",
    "predictions = []\n",
    "\n",
    "for text in test_texts:\n",
    "    encoding = TOKENIZER(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(probs, dim=1).item()\n",
    "        predictions.append((text, prediction, extract_mitre_ttp(text)))\n",
    "\n",
    "# Print sample results\n",
    "for sample in predictions[:5]:\n",
    "    print(f\"Text: {sample[0]}\\nThreat Intelligence: {bool(sample[1])}\\nMapped MITRE ATT&CK TTP: {sample[2]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
